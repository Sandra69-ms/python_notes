{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkifWrwtfJk57lJFbLwjZ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sandra69-ms/python_notes/blob/main/classification%2Clogistics_regression_note_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# classification  logistics regression"
      ],
      "metadata": {
        "id": "TLi-ArfGEK2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Classification\n",
        "\n"
      ],
      "metadata": {
        "id": "ziBXLz4zESOe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af0386ae"
      },
      "source": [
        "## Classification in Supervised Learning\n",
        "\n",
        "Classification is a supervised learning task where the goal is to predict a categorical label for a given input. This is different from regression, where the goal is to predict a continuous value.\n",
        "\n",
        "Some common examples of classification tasks include:\n",
        "\n",
        "* **Image classification:** Identifying the object in an image (e.g., cat, dog, car).\n",
        "* **Spam detection:** Classifying an email as spam or not spam.\n",
        "* **Medical diagnosis:** Predicting whether a patient has a particular disease based on their symptoms.\n",
        "\n",
        "Classification algorithms learn from a labeled dataset, which consists of input data and their corresponding correct labels. The algorithm uses this data to build a model that can predict the label for new, unseen data.\n",
        "\n",
        "Some popular classification algorithms include:\n",
        "\n",
        "* Logistic Regression\n",
        "* Support Vector Machines (SVM)\n",
        "* Decision Trees\n",
        "* Random Forests\n",
        "* k-Nearest Neighbors (k-NN)\n",
        "* Naive Bayes\n",
        "\n",
        "The choice of classification algorithm depends on the specific problem and the characteristics of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K0gdi5OtE_9R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd82b967"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "\n",
        "Logistic Regression is a statistical method used for **binary classification**, meaning it's used to predict one of two possible outcomes (like \"yes\" or \"no\", \"spam\" or \"not spam\").\n",
        "\n",
        "Think of it like trying to draw a line (or a curve in higher dimensions) to separate two groups of data points. Logistic regression finds the \"best\" line that separates these groups.\n",
        "\n",
        "Instead of directly predicting a category (like \"yes\" or \"no\"), logistic regression predicts the **probability** that a data point belongs to a certain category. This probability is then converted into a category based on a threshold (usually 0.5).\n",
        "\n",
        "It uses a special function called the **sigmoid function** to squeeze the output of a linear equation between 0 and 1, making it a probability.\n",
        "\n",
        "It's a simple yet powerful algorithm, often used as a baseline for classification problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9a21e7f"
      },
      "source": [
        "## Uses of Logistic Regression\n",
        "\n",
        "Logistic regression is a versatile algorithm used in various fields for binary classification tasks. Some common applications include:\n",
        "\n",
        "*   **Spam detection:** Classifying emails as spam or not spam.\n",
        "*   **Medical diagnosis:** Predicting the likelihood of a patient having a particular disease based on their symptoms and medical history.\n",
        "*   **Credit risk assessment:** Determining the probability of a loan applicant defaulting on a loan.\n",
        "*   **Marketing:** Predicting whether a customer will click on an advertisement or purchase a product.\n",
        "*   **Sentiment analysis:** Classifying the sentiment of text (e.g., positive or negative).\n",
        "*   **Image classification:** Simple binary image classification tasks, such as classifying an image as containing a cat or a dog."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53d83189"
      },
      "source": [
        "## Confusion Matrix\n",
        "\n",
        "A confusion matrix is a table that summarizes the performance of a classification model. It shows the number of true positive, true negative, false positive, and false negative predictions.\n",
        "\n",
        "\n",
        "\n",
        "*   **True Positive (TP):** The model correctly predicted the positive class.\n",
        "*   **True Negative (TN):** The model correctly predicted the negative class.\n",
        "*   **False Positive (FP):** The model incorrectly predicted the positive class (Type I error).\n",
        "*   **False Negative (FN):** The model incorrectly predicted the negative class (Type II error).\n",
        "\n",
        "A confusion matrix is a valuable tool for evaluating the performance of a classification model and understanding where it is making errors. It can be used to calculate various metrics, such as accuracy, precision, recall, and F1-score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "480421bb"
      },
      "source": [
        "## Accuracy Formula\n",
        "\n",
        "Accuracy is one of the most common metrics for evaluating classification models. It measures the overall proportion of correct predictions made by the model.\n",
        "\n",
        "The formula for accuracy is:\n",
        "\n",
        "$$ \\text{Accuracy} = \\frac{\\text{True Positives (TP)} + \\text{True Negatives (TN)}}{\\text{True Positives (TP)} + \\text{True Negatives (TN)} + \\text{False Positives (FP)} + \\text{False Negatives (FN)}} $$\n",
        "\n",
        "In simpler terms, accuracy is the number of correct predictions (both positive and negative) divided by the total number of predictions.\n",
        "\n",
        "While accuracy is a useful metric, it can be misleading in cases of imbalanced datasets (where one class has significantly more samples than the other). In such cases, other metrics like precision, recall, and F1-score might provide a better understanding of the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce5a012b"
      },
      "source": [
        "\n",
        "\n",
        "While accuracy provides an overall measure of correct predictions, precision, recall, and F1 score offer more nuanced insights into a classification model's performance, particularly when dealing with imbalanced datasets.\n",
        "\n",
        "These metrics are calculated using the values from the confusion matrix:\n",
        "\n",
        "*   **True Positive (TP):** The model correctly predicted the positive class.\n",
        "*   **True Negative (TN):** The model correctly predicted the negative class.\n",
        "*   **False Positive (FP):** The model incorrectly predicted the positive class (Type I error).\n",
        "*   **False Negative (FN):** The model incorrectly predicted the negative class (Type II error).\n",
        "\n",
        "Here are the formulas:\n",
        "\n",
        "### Precision\n",
        "\n",
        "Precision measures the proportion of correctly predicted positive instances among all instances predicted as positive. It answers the question: \"Of all the instances the model predicted as positive, how many were actually positive?\"\n",
        "\n",
        "$$ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} $$\n",
        "\n",
        "High precision indicates a low rate of false positives.\n",
        "\n",
        "### Recall (Sensitivity or True Positive Rate)\n",
        "\n",
        "Recall measures the proportion of correctly predicted positive instances among all actual positive instances. It answers the question: \"Of all the actual positive instances, how many did the model correctly identify?\"\n",
        "\n",
        "$$ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$\n",
        "\n",
        "High recall indicates a low rate of false negatives.\n",
        "\n",
        "### F1 Score\n",
        "\n",
        "The F1 Score is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall. It is particularly useful when you need to consider both false positives and false negatives.\n",
        "\n",
        "$$ \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
        "\n",
        "The F1 score ranges from 0 to 1, where 1 represents perfect precision and recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a671bec0"
      },
      "source": [
        "## K-Nearest Neighbors (KNN) Classification\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a simple, non-parametric lazy learning algorithm used for both classification and regression tasks. In the context of classification, KNN classifies a new data point based on the majority class of its 'k' nearest neighbors in the training data.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1.  **Choose a value for 'k':** This is a hyperparameter that determines the number of neighbors to consider.\n",
        "2.  **Calculate the distance:** For a new data point, calculate the distance (e.g., Euclidean distance) between it and all data points in the training set.\n",
        "3.  **Find the 'k' nearest neighbors:** Select the 'k' data points from the training set that have the smallest distances to the new data point.\n",
        "4.  **Determine the class:** Among the 'k' nearest neighbors, count the number of data points belonging to each class. The new data point is assigned to the class that is most frequent among its neighbors.\n",
        "\n",
        "**Key characteristics of KNN:**\n",
        "\n",
        "*   **Non-parametric:** It does not make any assumptions about the underlying data distribution.\n",
        "*   **Lazy learning:** It does not build a model during the training phase. All the work is done during the prediction phase when classifying a new data point.\n",
        "*   **Sensitive to the choice of 'k':** The value of 'k' can significantly impact the performance of the algorithm. A small 'k' can make the model sensitive to noise, while a large 'k' can smooth out the decision boundary but may also lead to misclassification of points near the boundaries.\n",
        "*   **Sensitive to the scale of features:** KNN uses distance metrics, so it's important to scale the features to prevent features with larger values from dominating the distance calculations.\n",
        "\n",
        "KNN is easy to understand and implement, making it a good starting point for classification problems. However, it can be computationally expensive for large datasets, especially during the prediction phase, as it needs to calculate the distance to all training points."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distance Metrics\n",
        "\n",
        "In machine learning, especially in algorithms like K-Nearest Neighbors (KNN) and clustering, we need to measure the similarity or dissimilarity between data points. This is done using **distance metrics**. A distance metric quantifies how \"far apart\" two data points are in a feature space.\n",
        "\n",
        "A **distance matrix** is a square matrix where each element $(i, j)$ represents the distance between the $i$-th and $j$-th data points in a dataset.\n",
        "\n",
        "There are several commonly used distance metrics. Here are a few:\n",
        "\n",
        "### Euclidean Distance\n",
        "\n",
        "This is the most common distance metric. It's the straight-line distance between two points in Euclidean space. For two points $p = (p_1, p_2, ..., p_n)$ and $q = (q_1, q_2, ..., q_n)$ in $n$-dimensional space, the Euclidean distance is calculated as:\n",
        "\n",
        "$$ d(p, q) = \\sqrt{\\sum_{i=1}^n (q_i - p_i)^2} $$\n",
        "\n",
        "### Manhattan Distance (or City Block Distance)\n",
        "\n",
        "This metric calculates the sum of the absolute differences between the coordinates of two points. It's like walking along the grid lines of a city. For two points $p$ and $q$:\n",
        "\n",
        "$$ d(p, q) = \\sum_{i=1}^n |q_i - p_i| $$\n",
        "\n",
        "### Minkowski Distance\n",
        "\n",
        "This is a generalization of Euclidean and Manhattan distances. For a parameter $p \\ge 1$:\n",
        "\n",
        "$$ d(p, q) = \\left(\\sum_{i=1}^n |q_i - p_i|^p\\right)^{1/p} $$\n",
        "\n",
        "*   When $p = 1$, it's the Manhattan distance.\n",
        "*   When $p = 2$, it's the Euclidean distance.\n",
        "\n",
        "### Hamming Distance\n",
        "\n",
        "This metric is used for categorical or binary data. It counts the number of positions at which the corresponding elements are different.\n",
        "\n",
        "The choice of distance metric depends on the type of data and the specific problem. It's important to consider the properties of each metric and how it aligns with the underlying structure of your data."
      ],
      "metadata": {
        "id": "ITlXIQAcQ_JI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXwAX5JaH6QL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}